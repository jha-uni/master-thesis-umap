{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import colorcet\n",
    "import umap\n",
    "import scanpy as sc\n",
    "import pickle\n",
    "import time\n",
    "import itertools\n",
    "import warnings\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score, KFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sc.read_h5ad(DATA_DIR /\"chu_2016.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset.X.toarray()\n",
    "labels = dataset.obs.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(labels).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_copy = labels.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(labels)\n",
    "labels = le.transform(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "#z-score (normalization on cell level)\n",
    "data = stats.zscore(data, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested cross-validation on unreduced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nested cross-validation\n",
    "#in: X (array), y (pd Series), classifier (string) , param_grid (dict)\n",
    "#out: yhat (pd Series)\n",
    "\n",
    "def nested_cv(X, y, classifier, param_grid, outer_folds=3, inner_folds=3):\n",
    "    \n",
    "    #create empty list for test index and yhat\n",
    "    ind = [] \n",
    "    yhat_list = []\n",
    "    count = 0\n",
    "    \n",
    "    outer_cv = KFold(n_splits=outer_folds, shuffle=True, random_state=1)\n",
    "    \n",
    "    for train_ind, test_ind in outer_cv.split(X):\n",
    "        \n",
    "        #split data into train and test\n",
    "        X_train, X_test = X[train_ind, :], X[test_ind, :]\n",
    "        y_train, y_test = y[train_ind], y[test_ind]\n",
    "        \n",
    "        #select classifier\n",
    "        if classifier == \"svm\":\n",
    "            model_to_tune = SVC(random_state=1)\n",
    "        if classifier == \"knn\":\n",
    "            model_to_tune = KNeighborsClassifier()\n",
    "        if classifier == \"rf\":\n",
    "            model_to_tune = RandomForestClassifier(random_state=1)\n",
    "        if classifier == \"mlr\":\n",
    "            model_to_tune = LogisticRegression(random_state=1)\n",
    "        if classifier == \"lda\":\n",
    "            model_to_tune = LinearDiscriminantAnalysis()\n",
    "        if classifier == \"qda\":\n",
    "            model_to_tune = QuadraticDiscriminantAnalysis()\n",
    "\n",
    "        #inner cross-validation\n",
    "        inner_cv = KFold(n_splits=inner_folds, shuffle=True, random_state=1)\n",
    "        grid = GridSearchCV(model_to_tune, param_grid, cv=inner_cv, refit=True, n_jobs=-1, scoring='accuracy')\n",
    "        result = grid.fit(X_train, y_train)\n",
    "        \n",
    "        #fit best model on training set\n",
    "        best_model = result.best_estimator_\n",
    "        \n",
    "        # predictions on test set\n",
    "        yhat = best_model.predict(X_test)\n",
    "        \n",
    "        #store yhat and test index for an outer fold \n",
    "        yhat_list.append(yhat)\n",
    "        ind.append(test_ind)\n",
    "        \n",
    "        count += 1\n",
    "        print(\"fold \"+str(count))\n",
    "        \n",
    "    yhat_list = np.concatenate(yhat_list)\n",
    "    ind = np.concatenate(ind)\n",
    "    pred = [a for b, a in sorted(zip(ind, yhat_list))]\n",
    "    pred = pd.Series(pred)\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "svm_raw_yhat = nested_cv(data, labels, \"svm\", {\"C\": [1,10], \"gamma\": [1,0.1, 0.01, 0.001, \"auto\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute accuracy and F1-score\n",
    "%%time\n",
    "svm_raw_accuracy = accuracy_score(labels, svm_raw_yhat)\n",
    "svm_raw_f1 = f1_score(labels, svm_raw_yhat, average=\"macro\")\n",
    "(svm_raw_accuracy, svm_raw_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save accuracy and F1-score\n",
    "with open('v2_output/svm_raw_f1_f1.pkl', 'wb') as f:\n",
    "    pickle.dump(svm_raw_f1, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save predictions\n",
    "with open('v2_output/svm_raw_yhat_f1.pkl', 'wb') as f:\n",
    "    pickle.dump(svm_raw_yhat, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "knn_raw_yhat = nested_cv(data, labels, \"knn\", {'n_neighbors': [2,5,15]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "knn_raw_accuracy = accuracy_score(labels, knn_raw_yhat)\n",
    "knn_raw_f1 = f1_score(labels, knn_raw_yhat, average=\"macro\")\n",
    "(knn_raw_accuracy, knn_raw_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('v2_output/knn_raw_acc.pkl', 'wb') as f:\n",
    "    pickle.dump(knn_raw_accuracy, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('v2_output/knn_raw_yhat_acc.pkl', 'wb') as f:\n",
    "    pickle.dump(knn_raw_yhat, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rf_raw_yhat = nested_cv(data, labels, \"rf\", {'max_features': [\"sqrt\", \"log2\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "rf_raw_accuracy = accuracy_score(labels, rf_raw_yhat)\n",
    "rf_raw_f1 = f1_score(labels, rf_raw_yhat, average=\"macro\")\n",
    "(rf_raw_accuracy, rf_raw_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('v2_output/rf_raw_acc.pkl', 'wb') as f:\n",
    "    pickle.dump(rf_raw_accuracy, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('v2_output/rf_raw_yhat_acc.pkl', 'wb') as f:\n",
    "    pickle.dump(rf_raw_yhat, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mlr_raw_yhat = nested_cv(data, labels, \"mlr\", {'penalty': [\"none\", \"l2\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mlr_raw_accuracy = accuracy_score(labels, mlr_raw_yhat)\n",
    "mlr_raw_f1 = f1_score(labels, mlr_raw_yhat, average=\"macro\")\n",
    "(mlr_raw_accuracy, mlr_raw_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('v2_output/mlr_raw_f1_f1.pkl', 'wb') as f:\n",
    "    pickle.dump(mlr_raw_f1, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('v2_output/mlr_raw_yhat_f1.pkl', 'wb') as f:\n",
    "    pickle.dump(mlr_raw_yhat, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#equal priors\n",
    "priors = [1/7]*7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lda_raw_yhat = nested_cv(data, labels, \"lda\", {'priors': [None,priors]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lda_raw_accuracy = accuracy_score(labels, lda_raw_yhat)\n",
    "lda_raw_f1 = f1_score(labels, lda_raw_yhat, average=\"macro\")\n",
    "(lda_raw_accuracy, lda_raw_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('v2_output/lda_raw_f1_f1.pkl', 'wb') as f:\n",
    "    pickle.dump(lda_raw_f1, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('v2_output/lda_raw_yhat_f1.pkl', 'wb') as f:\n",
    "    pickle.dump(lda_raw_yhat, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "qda_raw_yhat = nested_cv(data, labels, \"qda\", {'priors': [None,priors]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "qda_raw_accuracy = accuracy_score(labels, qda_raw_yhat)\n",
    "qda_raw_f1 = f1_score(labels, qda_raw_yhat, average=\"macro\")\n",
    "(qda_raw_accuracy, qda_raw_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('v2_output/qda_raw_f1_f1.pkl', 'wb') as f:\n",
    "    pickle.dump(qda_raw_f1, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('v2_output/qda_raw_yhat_f1.pkl', 'wb') as f:\n",
    "    pickle.dump(qda_raw_yhat, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested cross-validation on PCA embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in: X (array), y (pd Series), classifier (string) , para_grid (dict)\n",
    "#function stores accuracy and F1-scores in acc and f1 respectively\n",
    "\n",
    "def nested_cv_pca(X, y, dimensions, classifier, param_grid, acc, f1, outer_folds=3, inner_folds=3):\n",
    "    \n",
    "    #empty list for test index and yhat\n",
    "    ind = [] \n",
    "    yhat_list = []\n",
    "\n",
    "    \n",
    "    outer_cv = KFold(n_splits=outer_folds, shuffle=True, random_state=1)\n",
    "    \n",
    "    for train_ind, test_ind in outer_cv.split(X):\n",
    "         \n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        \n",
    "        #split data into train and test\n",
    "        X_train, X_test = X[train_ind, :], X[test_ind, :]\n",
    "        y_train, y_test = y[train_ind], y[test_ind]\n",
    "        \n",
    "        #select classifier\n",
    "        if classifier == \"svm\":\n",
    "            model_to_tune = SVC(random_state=1)\n",
    "        if classifier == \"knn\":\n",
    "            model_to_tune = KNeighborsClassifier()\n",
    "        if classifier == \"rf\":\n",
    "            model_to_tune = RandomForestClassifier(random_state=1)            \n",
    "        if classifier == \"mlr\":\n",
    "            model_to_tune = LogisticRegression(random_state=1)\n",
    "        if classifier == \"lda\":\n",
    "            model_to_tune = LinearDiscriminantAnalysis()\n",
    "        if classifier == \"qda\":\n",
    "            model_to_tune = QuadraticDiscriminantAnalysis()\n",
    "            \n",
    "        \n",
    "        #specify pca model\n",
    "        pca_model = PCA(n_components=dimensions)\n",
    "\n",
    "        #inner cross-validation\n",
    "        inner_cv = KFold(n_splits=inner_folds, shuffle=True, random_state=1)\n",
    "        \n",
    "        steps = [('pca', pca_model), ('classifier', model_to_tune)]\n",
    "        pipeline = Pipeline(steps)\n",
    "        \n",
    "        grid = GridSearchCV(pipeline, param_grid, cv=inner_cv, refit=True, n_jobs=-1, scoring='accuracy')\n",
    "        result = grid.fit(X_train, y_train)\n",
    "        \n",
    "        #fit best model on training set\n",
    "        best_model = result.best_estimator_\n",
    "        \n",
    "        # predictions on test set\n",
    "        yhat = best_model.predict(X_test)\n",
    "        \n",
    "        #store yhat and test index for an outer fold \n",
    "        yhat_list.append(yhat)\n",
    "        ind.append(test_ind)        \n",
    "        \n",
    "    yhat_list = np.concatenate(yhat_list)\n",
    "    ind = np.concatenate(ind)\n",
    "    pred = [a for b, a in sorted(zip(ind, yhat_list))]\n",
    "    pred = pd.Series(pred)\n",
    "    \n",
    "    acc.append(accuracy_score(labels, pred))\n",
    "    f1.append(f1_score(labels, pred, average=\"macro\"))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_dim = [1,2,10,25,40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty list for accuracy and f1-scores\n",
    "pca_acc = []\n",
    "pca_f1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(len(pca_dim)):\n",
    "    nested_cv_pca(data, labels, pca_dim[i],\n",
    "                   \"svm\", {'classifier__C': [1,10], 'classifier__gamma': [1,0.1, 0.01, 0.001, \"auto\"] }, \n",
    "                   pca_acc, pca_f1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('v2_output/svm_pca_accuracy.pkl', 'wb') as f:\n",
    "    pickle.dump(pca_acc, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('v2_output/svm_pca_f1_f1.pkl', 'wb') as f:\n",
    "    pickle.dump(pca_f1, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_acc = []\n",
    "pca_f1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(len(pca_dim)):\n",
    "    nested_cv_pca(data, labels, pca_dim[i],\n",
    "                   \"knn\", {'classifier__n_neighbors': [2,5,15]}, \n",
    "                   pca_acc, pca_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('v2_output/knn_pca_acc.pkl', 'wb') as f:\n",
    "    pickle.dump(pca_acc, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('v2_output/knn_pca_f1_f1.pkl', 'wb') as f:\n",
    "    pickle.dump(pca_f1, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_acc = []\n",
    "pca_f1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(len(pca_dim)):\n",
    "    nested_cv_pca(data, labels, pca_dim[i],\n",
    "                   \"rf\", {'classifier__max_features': [\"sqrt\", \"log2\"]}, \n",
    "                   pca_acc, pca_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('v2_output/rf_pca_acc.pkl', 'wb') as f:\n",
    "    pickle.dump(pca_acc, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('v2_output/rf_pca_f1_f1.pkl', 'wb') as f:\n",
    "    pickle.dump(pca_f1, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_acc = []\n",
    "pca_f1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(len(pca_dim)):\n",
    "    nested_cv_pca(data, labels, pca_dim[i],\n",
    "                   \"mlr\", {'classifier__penalty': [\"none\", \"l2\"]}, \n",
    "                   pca_acc, pca_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('v2_output/mlr_pca_accuracy.pkl', 'wb') as f:\n",
    "    pickle.dump(pca_acc, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('v2_output/mlr_pca_f1_f1.pkl', 'wb') as f:\n",
    "    pickle.dump(pca_f1, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_acc = []\n",
    "pca_f1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(len(pca_dim)):\n",
    "    nested_cv_pca(data, labels, pca_dim[i],\n",
    "                   \"lda\", {'classifier__priors': [None,priors]}, \n",
    "                   pca_acc, pca_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('v2_output/lda_pca_accuracy.pkl', 'wb') as f:\n",
    "    pickle.dump(pca_acc, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('v2_output/lda_pca_f1_f1.pkl', 'wb') as f:\n",
    "    pickle.dump(pca_f1, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "priors = [1/7]*7\n",
    "priors = np.array(priors)\n",
    "priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_acc = []\n",
    "pca_f1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(len(pca_dim)):\n",
    "    nested_cv_pca(data, labels, pca_dim[i],\n",
    "                   \"qda\", {'classifier__priors': [None,priors]}, \n",
    "                   pca_acc, pca_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('v2_output/qda_pca_accuracy.pkl', 'wb') as f:\n",
    "    pickle.dump(pca_acc, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('v2_output/qda_pca_f1_f1.pkl', 'wb') as f:\n",
    "    pickle.dump(pca_f1, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested cross-validation on UMAP embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(2678136)\n",
    "\n",
    "#Generate 20 random numbers and list them\n",
    "seed_list = random.sample(range(10**0, 10**6), 20)\n",
    "\n",
    "#for nested cross-validation only one seed is used\n",
    "seed=[seed_list[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim=[1,2,10,25,40]\n",
    "neigh=[2, 5, 15, 25, 50, 75, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create combinations of number of dimensions and number of neighbors \n",
    "a = [dim,neigh,seed]\n",
    "test_paramters=list(itertools.product(*a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nested cross-validation\n",
    "#in: X (array), y (pd Series), classifier (string) , para_grid (dict)\n",
    "#function stores predictions (dict_yhat) and most optimal parameters (dict_param)\n",
    "\n",
    "def nested_cv_umap(X, y, dimensions, neigbors, seed, classifier, param_grid, dict_yhat, dict_param,\n",
    "                   outer_folds=3, inner_folds=3):\n",
    "    \n",
    "    #empty list for test index and yhat\n",
    "    ind = [] \n",
    "    yhat_list = []\n",
    "    best_param = {}\n",
    "    count = 1\n",
    "    \n",
    "    outer_cv = KFold(n_splits=outer_folds, shuffle=True, random_state=1)\n",
    "    \n",
    "    for train_ind, test_ind in outer_cv.split(X):\n",
    "         \n",
    "        \n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        \n",
    "        #split data into train and test\n",
    "        X_train, X_test = X[train_ind, :], X[test_ind, :]\n",
    "        y_train, y_test = y[train_ind], y[test_ind]\n",
    "        \n",
    "        #select classifier\n",
    "        if classifier == \"svm\":\n",
    "            model_to_tune = SVC(random_state=1)\n",
    "        if classifier == \"knn\":\n",
    "            model_to_tune = KNeighborsClassifier()\n",
    "        if classifier == \"rf\":\n",
    "            model_to_tune = RandomForestClassifier(random_state=1)\n",
    "        if classifier == \"mlr\":\n",
    "            model_to_tune = LogisticRegression(random_state=1)\n",
    "        if classifier == \"lda\":\n",
    "            model_to_tune = LinearDiscriminantAnalysis()\n",
    "        if classifier == \"qda\":\n",
    "            model_to_tune = QuadraticDiscriminantAnalysis()\n",
    "            \n",
    "        \n",
    "        #specify umap model\n",
    "        umap_model = umap.UMAP(n_components=dimensions, n_neighbors=neigbors, random_state=seed)\n",
    "\n",
    "        #inner cross-validation\n",
    "        inner_cv = KFold(n_splits=inner_folds, shuffle=True, random_state=1)\n",
    "        \n",
    "        steps = [('umap', umap_model), ('classifier', model_to_tune)]\n",
    "        pipeline = Pipeline(steps)\n",
    "        \n",
    "        grid = GridSearchCV(pipeline, param_grid, cv=inner_cv, refit=True, n_jobs=-1, scoring='accuracy')\n",
    "        result = grid.fit(X_train, y_train)\n",
    "        \n",
    "        #fit best model on training set\n",
    "        best_model = result.best_estimator_\n",
    "        \n",
    "        # predictions on test set\n",
    "        yhat = best_model.predict(X_test)\n",
    "        \n",
    "        #store yhat and test index for an outer fold \n",
    "        yhat_list.append(yhat)\n",
    "        ind.append(test_ind)\n",
    "        \n",
    "        #store best model\n",
    "        best_param[str(count)] = best_model\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "        \n",
    "    yhat_list = np.concatenate(yhat_list)\n",
    "    ind = np.concatenate(ind)\n",
    "    pred = [a for b, a in sorted(zip(ind, yhat_list))]\n",
    "    pred = pd.Series(pred)\n",
    "    \n",
    "    key = \"d\"+str(dimensions)+\"n\"+str(neigbors)\n",
    "    dict_yhat[key] = pred\n",
    "    \n",
    "    dict_param[key] = best_param\n",
    "    \n",
    "    print(key)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty list for predictions and best parameters\n",
    "dict_yhat = {}\n",
    "dict_param = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run function for every hyperparameter combination\n",
    "%%time\n",
    "for i in range(len(test_paramters)):\n",
    "    nested_cv_umap(data, labels, test_paramters[i][0], test_paramters[i][1], test_paramters[i][2], \n",
    "                   \"svm\", {'classifier__C': [1,10], 'classifier__gamma': [1,0.1, 0.01, 0.001, \"auto\"] }, \n",
    "                   dict_yhat, dict_param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dictionary with predictions for every hyperparameter combination\n",
    "with open('v2_output/svm_umap_yhat_0_f1.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_yhat, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dictionary with most optimal parameters in each fold\n",
    "test_keys = list(dict_param.keys())\n",
    "dict_params = {}\n",
    "\n",
    "for i in range(len(test_keys)):\n",
    "    dict_params_inner = {}\n",
    "\n",
    "    for j in range(1,4):\n",
    "        dict_params_inner[str(j)] = dict_param[str(test_keys[i])][str(j)][1]\n",
    "        \n",
    "    dict_params[str(test_keys[i])] =  dict_params_inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('v2_output/svm_umap_params_0_f1.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_params, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_yhat = {}\n",
    "dict_param = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(len(test_paramters)):\n",
    "    nested_cv_umap(data, labels, test_paramters[i][0], test_paramters[i][1], test_paramters[i][2],\n",
    "                   \"knn\", {'classifier__n_neighbors': [2,5,15]}, dict_yhat, dict_param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('v2_output/knn_umap_yhat_0_acc.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_yhat, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_keys = list(dict_param.keys())\n",
    "dict_params = {}\n",
    "\n",
    "for i in range(len(test_keys)):\n",
    "    dict_params_inner = {}\n",
    "\n",
    "    for j in range(1,4):\n",
    "        dict_params_inner[str(j)] = dict_param[str(test_keys[i])][str(j)][1]\n",
    "        \n",
    "    dict_params[str(test_keys[i])] =  dict_params_inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('v2_output/knn_umap_params_0_acc.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_params, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_yhat = {}\n",
    "dict_param = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(len(test_paramters)):\n",
    "    nested_cv_umap(data, labels, test_paramters[i][0], test_paramters[i][1], test_paramters[i][2],\n",
    "                   \"rf\", {'classifier__max_features': [\"sqrt\", \"log2\"]}, dict_yhat, dict_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('v2_output/rf_umap_yhat_0_acc.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_yhat, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_keys = list(dict_param.keys())\n",
    "dict_params = {}\n",
    "\n",
    "for i in range(len(test_keys)):\n",
    "    dict_params_inner = {}\n",
    "\n",
    "    for j in range(1,4):\n",
    "        dict_params_inner[str(j)] = dict_param[str(test_keys[i])][str(j)][1]\n",
    "        \n",
    "    dict_params[str(test_keys[i])] =  dict_params_inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('v2_output/rf_umap_params_0_acc.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_params, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_yhat = {}\n",
    "dict_param = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(len(test_paramters)):\n",
    "    nested_cv_umap(data, labels, test_paramters[i][0], test_paramters[i][1], test_paramters[i][2],\n",
    "                   \"mlr\", {'classifier__penalty': [\"none\", \"l2\"]}, dict_yhat, dict_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('v2_output/mlr_umap_yhat_0_f1.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_yhat, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_keys = list(dict_param.keys())\n",
    "dict_params = {}\n",
    "\n",
    "for i in range(len(test_keys)):\n",
    "    dict_params_inner = {}\n",
    "\n",
    "    for j in range(1,4):\n",
    "        dict_params_inner[str(j)] = dict_param[str(test_keys[i])][str(j)][1]\n",
    "        \n",
    "    dict_params[str(test_keys[i])] =  dict_params_inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('v2_output/mlr_umap_params_0_f1.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_params, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_yhat = {}\n",
    "dict_param = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(len(test_paramters)):\n",
    "    nested_cv_umap(data, labels, test_paramters[i][0], test_paramters[i][1],\n",
    "                       test_paramters[i][2], \"lda\", {'classifier__priors': [None,priors]}, dict_yhat, dict_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('v2_output/lda_umap_yhat_0_f1.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_yhat, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_keys = list(dict_param.keys())\n",
    "dict_params = {}\n",
    "\n",
    "for i in range(len(test_keys)):\n",
    "    dict_params_inner = {}\n",
    "\n",
    "    for j in range(1,4):\n",
    "        dict_params_inner[str(j)] = dict_param[str(test_keys[i])][str(j)][1]\n",
    "        \n",
    "    dict_params[str(test_keys[i])] =  dict_params_inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('v2_output/lda_umap_params_0_f1.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_params, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "priors = [1/7]*7\n",
    "priors = np.array(priors)\n",
    "priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_yhat = {}\n",
    "dict_param = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(len(test_paramters)):\n",
    "    nested_cv_umap(data, labels, test_paramters[i][0], test_paramters[i][1],\n",
    "                       test_paramters[i][2], \"qda\", {'classifier__priors': [None,priors]}, dict_yhat, dict_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('v2_output/qda_umap_yhat_0_f1.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_yhat, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_keys = list(dict_param.keys())\n",
    "dict_params = {}\n",
    "\n",
    "for i in range(len(test_keys)):\n",
    "    dict_params_inner = {}\n",
    "\n",
    "    for j in range(1,4):\n",
    "        dict_params_inner[str(j)] = dict_param[str(test_keys[i])][str(j)][1]\n",
    "        \n",
    "    dict_params[str(test_keys[i])] =  dict_params_inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('v2_output/qda_umap_params_0_f1.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_params, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute accuracy and F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dictionary with predictions\n",
    "with open('v2_output/knn_umap_yhat_0_acc.pkl', 'rb') as f:\n",
    "    dict_yhat = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_acc = {}\n",
    "dict_f1 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function computes accuracy and f1-scores and store them in dict_acc and dict_f1\n",
    "def compute_dict_scores(key, dict_yhat, dict_acc, dict_f1, labels ):\n",
    "    \n",
    "    dict_acc[key] = accuracy_score(labels, dict_yhat[key])\n",
    "    dict_f1[key] = f1_score(labels, dict_yhat[key], average=\"macro\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_parameters_accuracy = list(dict_yhat.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run function for every hyperparameter combination\n",
    "for i in range(len(test_parameters_accuracy)):\n",
    "    compute_dict_scores(test_parameters_accuracy[i], dict_yhat, dict_acc, dict_f1, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dictionary with accuracy/f1 score for every hyperparameter combination (key)\n",
    "with open('v2_output/knn_umap_acc_opt.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_acc, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot nested CV results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dictionary with accuracy/f1 score for every hyperparameter combination (key)\n",
    "with open('v2_output/lda_umap_f1_f1.pkl', 'rb') as f:\n",
    "    dict_ = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table(scores):\n",
    "    values = {\n",
    "        'n2': [scores['d1n2'], scores['d2n2'], scores['d10n2'], scores['d25n2'], scores['d40n2']],\n",
    "        'n5': [scores['d1n5'], scores['d2n5'], scores['d10n5'], scores['d25n5'], scores['d40n5']],\n",
    "        'n15': [scores['d1n15'], scores['d2n15'], scores['d10n15'], scores['d25n15'], scores['d40n15']],\n",
    "        'n25': [scores['d1n25'], scores['d2n25'], scores['d10n25'], scores['d25n25'], scores['d40n25']],\n",
    "        'n50': [scores['d1n50'], scores['d2n50'], scores['d10n50'],scores['d25n50'], scores['d40n50']],\n",
    "        'n75': [scores['d1n75'], scores['d2n75'], scores['d10n75'], scores['d25n75'], scores['d40n75']],\n",
    "        'n100': [scores['d1n100'], scores['d2n100'], scores['d10n100'], scores['d25n100'], scores['d40n100']]\n",
    "                }\n",
    "    \n",
    "    table = pd.DataFrame(values, index=['d1','d2','d10','d25','d40'])\n",
    "\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#table with accuracy/f1-score for umap (dimensions xneighbors)\n",
    "table_acc = create_table(dict_)\n",
    "table_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plots accuracy/f1 score for umap, pca and unreduced in one figure\n",
    "def plot(table, accuracy_raw, accuracy_pca, name, path):\n",
    "    \n",
    "    t=np.array([2, 5, 15, 25, 50, 75, 100])\n",
    "    #labels=['d1','d2','d10', 'd25', 'd40', 'raw']\n",
    "    labels=['d1','d2','d10', 'd25', 'd40', 'pca1','pca2','pca10', 'pca25', 'pca40', 'raw']\n",
    "    \n",
    "    plt.figure(figsize = (7,4) )\n",
    "    ax = plt.gca()\n",
    "    ax.plot(t, table.iloc[0,:], color = 'red', label=labels[0], lw=1.5, marker='.')\n",
    "    \n",
    "    ax.plot(t, table.iloc[1,:], color = 'orange', label=labels[1], lw=1.5, marker='.')\n",
    "    \n",
    "    ax.plot(t, table.iloc[2,:], color = 'green', label=labels[2], lw=1.5, marker='.')\n",
    "    \n",
    "    ax.plot(t, table.iloc[3,:], color = 'dodgerblue', label=labels[3], lw=1.5, marker='.')\n",
    "    \n",
    "    ax.plot(t, table.iloc[4,:], color = 'violet', label=labels[4], lw=1.5, marker='.')\n",
    "    \n",
    "    plt.axhline(y=accuracy_pca[0], color='red', linestyle='--', label=labels[5])\n",
    "    plt.axhline(y=accuracy_pca[1], color='orange', linestyle='--', label=labels[6])\n",
    "    plt.axhline(y=accuracy_pca[2], color='green', linestyle='--', label=labels[7])\n",
    "    plt.axhline(y=accuracy_pca[3], color='dodgerblue', linestyle='--', label=labels[8])\n",
    "    plt.axhline(y=accuracy_pca[4], color='violet', linestyle='--', label=labels[9])\n",
    "    \n",
    "    plt.axhline(y=accuracy_raw, color='gray', linestyle='-.', label=labels[10])\n",
    "    \n",
    "    plt.ylim(0.5, 1.01)\n",
    "    #plt.ylim(0, 1.01)\n",
    "    \n",
    "    plt.xlabel(\"Number of neigbors\")\n",
    "    #plt.ylabel(\"Accuracy\")\n",
    "    plt.ylabel(\"F1 score\")\n",
    "    \n",
    "    plt.savefig(path, facecolor='white', bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load accuracy/f1 score for unreduced dataset\n",
    "with open('v2_output/lda_raw_f1_f1.pkl', 'rb') as f:\n",
    "    raw = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load accuracy/f1 score for PCA embeddings\n",
    "with open('v2_output/lda_pca_f1_f1.pkl', 'rb') as f:\n",
    "    pca = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(table_acc, raw, pca, '-', 'v2_output/lda_plot_f1_f1_w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non cross-validated analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=.2, random_state=2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance on unreduced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=100)\n",
    "knn.fit(x_train, y_train)\n",
    "knn_raw_yhat = knn.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute accuracy and F1-score\n",
    "knn_raw_accuracy = accuracy_score(y_test, knn_raw_yhat)\n",
    "knn_raw_f1 = f1_score(y_test, knn_raw_yhat, average=\"macro\")\n",
    "(knn_raw_accuracy, knn_raw_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('v2_output/knn100_raw_accuracy.pkl', 'wb') as f:\n",
    "    pickle.dump(knn_raw_accuracy, f)\n",
    "    \n",
    "with open('v2_output/knn100_raw_f1.pkl', 'wb') as f:\n",
    "    pickle.dump(knn_raw_f1, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('v2_output/knn100_raw_yhat.pkl', 'wb') as f:\n",
    "    pickle.dump(knn_raw_yhat, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance on UMAP embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute training and test embeddings\n",
    "#embeddings are stored in dict_emb\n",
    "\n",
    "def compute_embeddings(x_train, x_test, dimensions, neigbors, seed, dict_emb):\n",
    "    \n",
    "    \n",
    "    embedder = umap.UMAP(n_neighbors=neigbors, n_components=dimensions, random_state=seed).fit(x_train)\n",
    "    \n",
    "    train_embedding = embedder.embedding_\n",
    "    \n",
    "    test_embedding = embedder.transform(x_test)\n",
    "    \n",
    "    key = \"d\"+str(dimensions)+\"n\"+str(neigbors)\n",
    "    \n",
    "    dict_emb[key] = (train_embedding, test_embedding)\n",
    "    \n",
    "    \n",
    "    print(key)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the same list with random numbers as before\n",
    "seed_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim=[1,2,10,25,40]\n",
    "neigh=[2, 5, 15, 25, 50, 75, 100]\n",
    "#test=[1]\n",
    "seed=[seed_list[9]]\n",
    "seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create combinations of number of dimensions and number of neighbors \n",
    "#repeat for 10 different random seeds\n",
    "a = [dim,neigh,seed]\n",
    "test_paramters=list(itertools.product(*a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty dictionary to store embeddings\n",
    "dict_emb = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#repeat for 10 different random seeds\n",
    "%%time\n",
    "for i in range(len(test_paramters)):\n",
    "    compute_embeddings(x_train, x_test, test_paramters[i][0], test_paramters[i][1],\n",
    "                       test_paramters[i][2], dict_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dictionary which contains a training embedding and test embeddind for every hyperparameter combination\n",
    "#repeat for 10 different seeds\n",
    "with open('v2_output/Chu_emb_9.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_emb, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute accuracy and F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty dictionaries to store accuracy and F1 scores\n",
    "dict_knn2_acc = {}\n",
    "dict_knn5_acc = {}\n",
    "dict_knn15_acc = {}\n",
    "dict_knn25_acc = {}\n",
    "dict_knn50_acc = {}\n",
    "dict_knn75_acc = {}\n",
    "dict_knn100_acc = {}\n",
    "\n",
    "dict_knn2_f1 = {}\n",
    "dict_knn5_f1 = {}\n",
    "dict_knn15_f1 = {}\n",
    "dict_knn25_f1 = {}\n",
    "dict_knn50_f1 = {}\n",
    "dict_knn75_f1 = {}\n",
    "dict_knn100_f1 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit classification models and compute accuracy and f1 scores\n",
    "\n",
    "def compute_accuracy_scores(key, train_embedding, test_embedding, y_train, y_test, \n",
    "                            knn5_acc, knn15_acc, \n",
    "                            knn25_acc, knn50_acc, knn75_acc, knn100_acc,\n",
    "                            knn2_f1, knn5_f1, knn15_f1, \n",
    "                            knn25_f1, knn50_f1, knn75_f1, knn100_f1):\n",
    "    \n",
    "\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    \n",
    "    #fit classifiers\n",
    "\n",
    "    knn2_umap = KNeighborsClassifier(n_neighbors=2)\n",
    "    knn2_umap.fit(train_embedding, y_train)\n",
    "    knn2_yhat = knn2_umap.predict(test_embedding)\n",
    "    \n",
    "    knn5_umap = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn5_umap.fit(train_embedding, y_train)\n",
    "    knn5_yhat = knn5_umap.predict(test_embedding)\n",
    "    \n",
    "    knn15_umap = KNeighborsClassifier(n_neighbors=15)\n",
    "    knn15_umap.fit(train_embedding, y_train)\n",
    "    knn15_yhat = knn15_umap.predict(test_embedding)\n",
    "    \n",
    "    knn25_umap = KNeighborsClassifier(n_neighbors=25)\n",
    "    knn25_umap.fit(train_embedding, y_train)\n",
    "    knn25_yhat = knn25_umap.predict(test_embedding)\n",
    "    \n",
    "    knn50_umap = KNeighborsClassifier(n_neighbors=50)\n",
    "    knn50_umap.fit(train_embedding, y_train)\n",
    "    knn50_yhat = knn50_umap.predict(test_embedding)\n",
    "\n",
    "    knn75_umap = KNeighborsClassifier(n_neighbors=75)\n",
    "    knn75_umap.fit(train_embedding, y_train)\n",
    "    knn75_yhat = knn75_umap.predict(test_embedding)\n",
    "\n",
    "    knn100_umap = KNeighborsClassifier(n_neighbors=100)\n",
    "    knn100_umap.fit(train_embedding, y_train)\n",
    "    knn100_yhat = knn100_umap.predict(test_embedding)\n",
    "\n",
    " \n",
    "    #compute scores\n",
    "    \n",
    "\n",
    "    knn2_acc[key] = [accuracy_score(y_test, knn2_yhat)]\n",
    "    knn2_f1[key] = [f1_score(y_test, knn2_yhat, average=\"macro\")]\n",
    "\n",
    "    knn5_acc[key] = [accuracy_score(y_test, knn5_yhat)]\n",
    "    knn5_f1[key] = [f1_score(y_test, knn5_yhat, average=\"macro\")]\n",
    "\n",
    "    knn15_acc[key] = [accuracy_score(y_test, knn15_yhat)]\n",
    "    knn15_f1[key] = [f1_score(y_test, knn15_yhat, average=\"macro\")]\n",
    "\n",
    "    knn25_acc[key] = [accuracy_score(y_test, knn25_yhat)]\n",
    "    knn25_f1[key] = [f1_score(y_test, knn25_yhat, average=\"macro\")]\n",
    "\n",
    "    knn50_acc[key] = [accuracy_score(y_test, knn50_yhat)]\n",
    "    knn50_f1[key] = [f1_score(y_test, knn50_yhat, average=\"macro\")]\n",
    "\n",
    "    knn75_acc[key] = [accuracy_score(y_test, knn75_yhat)]\n",
    "    knn75_f1[key] = [f1_score(y_test, knn75_yhat, average=\"macro\")]\n",
    "\n",
    "    knn100_acc[key] = [accuracy_score(y_test, knn100_yhat)]\n",
    "    knn100_f1[key] = [f1_score(y_test, knn100_yhat, average=\"macro\")]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit classification models and compute accuracy and f1 scores\n",
    "\n",
    "def compute_accuracy_scores(key, train_embedding, test_embedding, y_train, y_test, \n",
    "                            knn2_acc, knn5_acc, knn15_acc, \n",
    "                            knn25_acc, knn50_acc, knn75_acc, knn100_acc,\n",
    "                            knn2_f1, knn5_f1, knn15_f1, \n",
    "                            knn25_f1, knn50_f1, knn75_f1, knn100_f1):\n",
    "    \n",
    "\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    \n",
    "    #fit classifiers\n",
    "\n",
    "    \n",
    "    knn2_umap = KNeighborsClassifier(n_neighbors=2)\n",
    "    knn2_umap.fit(train_embedding, y_train)\n",
    "    knn2_yhat = knn2_umap.predict(test_embedding)\n",
    "    \n",
    "    knn5_umap = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn5_umap.fit(train_embedding, y_train)\n",
    "    knn5_yhat = knn5_umap.predict(test_embedding)\n",
    "    \n",
    "    knn15_umap = KNeighborsClassifier(n_neighbors=15)\n",
    "    knn15_umap.fit(train_embedding, y_train)\n",
    "    knn15_yhat = knn15_umap.predict(test_embedding)\n",
    "    \n",
    "    knn25_umap = KNeighborsClassifier(n_neighbors=25)\n",
    "    knn25_umap.fit(train_embedding, y_train)\n",
    "    knn25_yhat = knn25_umap.predict(test_embedding)\n",
    "    \n",
    "    knn50_umap = KNeighborsClassifier(n_neighbors=50)\n",
    "    knn50_umap.fit(train_embedding, y_train)\n",
    "    knn50_yhat = knn50_umap.predict(test_embedding)\n",
    "\n",
    "    knn75_umap = KNeighborsClassifier(n_neighbors=75)\n",
    "    knn75_umap.fit(train_embedding, y_train)\n",
    "    knn75_yhat = knn75_umap.predict(test_embedding)\n",
    "\n",
    "    knn100_umap = KNeighborsClassifier(n_neighbors=100)\n",
    "    knn100_umap.fit(train_embedding, y_train)\n",
    "    knn100_yhat = knn100_umap.predict(test_embedding)\n",
    "\n",
    " \n",
    "    #compute scores\n",
    "\n",
    "\n",
    "    knn2_acc[key].append(accuracy_score(y_test, knn2_yhat))\n",
    "    knn2_f1[key].append(f1_score(y_test, knn2_yhat, average=\"macro\"))\n",
    "\n",
    "    knn5_acc[key].append(accuracy_score(y_test, knn5_yhat))\n",
    "    knn5_f1[key].append(f1_score(y_test, knn5_yhat, average=\"macro\"))\n",
    "\n",
    "    knn15_acc[key].append(accuracy_score(y_test, knn15_yhat))\n",
    "    knn15_f1[key].append(f1_score(y_test, knn15_yhat, average=\"macro\"))\n",
    "\n",
    "    knn25_acc[key].append(accuracy_score(y_test, knn25_yhat))\n",
    "    knn25_f1[key].append(f1_score(y_test, knn25_yhat, average=\"macro\"))\n",
    "\n",
    "    knn50_acc[key].append(accuracy_score(y_test, knn50_yhat))\n",
    "    knn50_f1[key].append(f1_score(y_test, knn50_yhat, average=\"macro\"))\n",
    "\n",
    "    knn75_acc[key].append(accuracy_score(y_test, knn75_yhat))\n",
    "    knn75_f1[key].append(f1_score(y_test, knn75_yhat, average=\"macro\"))\n",
    "\n",
    "    knn100_acc[key].append(accuracy_score(y_test, knn100_yhat))\n",
    "    knn100_f1[key].append(f1_score(y_test, knn100_yhat, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "test_parameters_accuracy = list(dict_emb.keys())\n",
    "test_parameters_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('v2_output/Chu_emb_9.pkl', 'rb') as f:\n",
    "    dict_emb = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run function for every hyperparameter combinations\n",
    "for i in range(len(test_parameters_accuracy)):\n",
    "    compute_accuracy_scores(test_parameters_accuracy[i], dict_emb[test_parameters_accuracy[i]][0], \n",
    "                       dict_emb[test_parameters_accuracy[i]][1], \n",
    "                       y_train, y_test, \n",
    "                       dict_knn2_acc, dict_knn5_acc, dict_knn15_acc, \n",
    "                        dict_knn25_acc, dict_knn50_acc, dict_knn75_acc, dict_knn100_acc,\n",
    "                        dict_knn2_f1, dict_knn5_f1, dict_knn15_f1, \n",
    "                        dict_knn25_f1, dict_knn50_f1, dict_knn75_f1, dict_knn100_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dictionaries which stores 10 performance scores for each hyperparameter combination\n",
    "with open('v2_output/knn2_umap_acc.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_knn2_acc, f)\n",
    "with open('v2_output/knn2_umap_f1.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_knn2_f1, f)\n",
    "    \n",
    "with open('v2_output/knn5_umap_acc.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_knn5_acc, f)\n",
    "with open('v2_output/knn5_umap_f1.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_knn5_f1, f)\n",
    "    \n",
    "with open('v2_output/knn15_umap_acc.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_knn15_acc, f)\n",
    "with open('v2_output/knn15_umap_f1.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_knn15_f1, f)\n",
    "    \n",
    "with open('v2_output/knn25_umap_acc.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_knn25_acc, f)\n",
    "with open('v2_output/knn25_umap_f1.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_knn25_f1, f)\n",
    "    \n",
    "with open('v2_output/knn50_umap_acc.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_knn50_acc, f)\n",
    "with open('v2_output/knn50_umap_f1.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_knn50_f1, f)\n",
    "    \n",
    "with open('v2_output/knn75_umap_acc.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_knn75_acc, f)\n",
    "with open('v2_output/knn75_umap_f1.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_knn75_f1, f)\n",
    "    \n",
    "with open('v2_output/knn100_umap_acc.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_knn100_acc, f)\n",
    "with open('v2_output/knn100_umap_f1.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_knn100_f1, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('v2_output/qda_umap_acc.pkl', 'rb') as f:\n",
    "    dict_acc = pickle.load(f)\n",
    "\n",
    "with open('v2_output/qda_umap_f1.pkl', 'rb') as f:\n",
    "    dict_f1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acc = pd.DataFrame(dict_acc)\n",
    "\n",
    "df_f1 = pd.DataFrame(dict_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_acc_mean = dict(df_acc.describe().loc['mean'])\n",
    "dict_acc_50 = dict(df_acc.describe().loc['50%'])\n",
    "dict_acc_25 = dict(df_acc.describe().loc['25%'])\n",
    "dict_acc_75 = dict(df_acc.describe().loc['75%'])\n",
    "\n",
    "dict_f1_mean = dict(df_f1.describe().loc['mean'])\n",
    "dict_f1_50 = dict(df_f1.describe().loc['50%'])\n",
    "dict_f1_25 = dict(df_f1.describe().loc['25%'])\n",
    "dict_f1_75 = dict(df_f1.describe().loc['75%'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table(scores):\n",
    "    values = {\n",
    "        'n2': [scores['d1n2'], scores['d2n2'], scores['d10n2'], scores['d25n2'], scores['d40n2']],\n",
    "        'n5': [scores['d1n5'], scores['d2n5'], scores['d10n5'], scores['d25n5'], scores['d40n5']],\n",
    "        'n15': [scores['d1n15'], scores['d2n15'], scores['d10n15'], scores['d25n15'], scores['d40n15']],\n",
    "        'n25': [scores['d1n25'], scores['d2n25'], scores['d10n25'], scores['d25n25'], scores['d40n25']],\n",
    "        'n50': [scores['d1n50'], scores['d2n50'], scores['d10n50'],scores['d25n50'], scores['d40n50']],\n",
    "        'n75': [scores['d1n75'], scores['d2n75'], scores['d10n75'], scores['d25n75'], scores['d40n75']],\n",
    "        'n100': [scores['d1n100'], scores['d2n100'], scores['d10n100'], scores['d25n100'], scores['d40n100']]\n",
    "                }\n",
    "    \n",
    "    table = pd.DataFrame(values, index=['d1','d2','d10','d25','d40'])\n",
    "\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_acc_mean = create_table(dict_acc_mean)\n",
    "table_acc_50 = create_table(dict_acc_50)\n",
    "table_acc_25 = create_table(dict_acc_25)\n",
    "table_acc_75 = create_table(dict_acc_75)\n",
    "\n",
    "table_f1_mean = create_table(dict_f1_mean)\n",
    "table_f1_50 = create_table(dict_f1_50)\n",
    "table_f1_25 = create_table(dict_f1_25)\n",
    "table_f1_75 = create_table(dict_f1_75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_iqr_knn(table_50, table_25, table_75, accuracy_raw, path):\n",
    "    \n",
    "    t=np.array([1, 2, 10, 25, 40])\n",
    "    labels=['n2','n5','n15', 'n25', 'n50', 'n75', 'n100', 'raw']\n",
    "    \n",
    "    \n",
    "    ax = plt.gca()\n",
    "    ax.plot(t, table_50.iloc[0,:], color = 'red', label=labels[0], lw=1.5, marker='.')\n",
    "    ax.fill_between(t, table_25.iloc[0,:], table_75.iloc[0,:], color = 'red', alpha=0.2, lw=0)\n",
    "    \n",
    "    ax.plot(t, table_50.iloc[1,:], color = 'orange', label=labels[1], lw=1.5, marker='.')\n",
    "    ax.fill_between(t, table_25.iloc[1,:], table_75.iloc[1,:], color = 'orange', alpha=0.2, lw=0)\n",
    "    \n",
    "    ax.plot(t, table_50.iloc[2,:], color = 'green', label=labels[2], lw=1.5, marker='.')\n",
    "    ax.fill_between(t, table_25.iloc[2,:], table_75.iloc[2,:], color = 'green', alpha=0.2, lw=0)\n",
    "    \n",
    "    ax.plot(t, table_50.iloc[3,:], color = 'dodgerblue', label=labels[3], lw=1.5, marker='.')\n",
    "    ax.fill_between(t, table_25.iloc[3,:], table_75.iloc[3,:], color = 'dodgerblue', alpha=0.2, lw=0)\n",
    "    \n",
    "    ax.plot(t, table_50.iloc[4,:], color = 'violet', label=labels[4], lw=1.5, marker='.')\n",
    "    ax.fill_between(t, table_25.iloc[4,:], table_75.iloc[4,:], color = 'violet', alpha=0.2, lw=0)\n",
    "    \n",
    "    ax.plot(t, table_50.iloc[5,:], color = 'slateblue', label=labels[5], lw=1.5, marker='.')\n",
    "    ax.fill_between(t, table_25.iloc[5,:], table_75.iloc[5,:], color = 'slateblue', alpha=0.2, lw=0)\n",
    "    \n",
    "    ax.plot(t, table_50.iloc[6,:], color = 'brown', label=labels[6], lw=1.5, marker='.')\n",
    "    ax.fill_between(t, table_25.iloc[6,:], table_75.iloc[6,:], color = 'brown', alpha=0.2, lw=0)\n",
    "    \n",
    "    \n",
    "    plt.axhline(y=accuracy_raw, color='gray', linestyle='-.', label=labels[7])\n",
    "    \n",
    "    \n",
    "    plt.ylim(0.5, 1.01)\n",
    "    \n",
    "    plt.xlabel(\"Number of dimensions\")\n",
    "    #plt.ylabel(\"Accuracy\")\n",
    "    plt.ylabel(\"F1 score\")\n",
    "    legend_outside = plt.legend(bbox_to_anchor=(1.20,0.72), loc='right')\n",
    "\n",
    "    \n",
    "    plt.savefig(path, facecolor='white', bbox_inches='tight', dpi=300)\n",
    "    #plt.show()    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('v2_output/knn5_raw_f1.pkl', 'rb') as f:\n",
    "    raw = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_iqr(table_f1_mean, table_f1_25, table_f1_75, \n",
    "         raw, 'v2_output/knn5_plot_f13')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_testt = le.inverse_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('v2_output/pc2.pkl', 'rb') as f:\n",
    "    pc = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('v2_output/Chu_emb_0.pkl', 'rb') as f:\n",
    "    emb = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelss = le.inverse_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_legend = list(labels_copy.unique())\n",
    "labels_legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pal = sns.color_palette()\n",
    "color = pal.as_hex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_dict = {}\n",
    "keys = labels_legend\n",
    "values = color\n",
    "for i in range(len(keys)):\n",
    "    color_dict[keys[i]] = values[i]\n",
    "print(color_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training vs test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trainn = le.inverse_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subplots\n",
    "\n",
    "# create figure\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 10), sharey=False)\n",
    "\n",
    "\n",
    "# add subplots\n",
    "#plt.figure(figsize = (10,10) )\n",
    "sns.scatterplot(ax=axes[0], x=emb['d2n5'][0][:,0], y=emb['d2n5'][0][:,1], hue = y_trainn, palette=color_dict)\n",
    "axes[0].axis('equal')\n",
    "axes[0].set_title(\"Embedding of training data\", fontsize=45)\n",
    "axes[0].tick_params(axis='both', which='major', labelsize=22)\n",
    "\n",
    "sns.scatterplot(ax=axes[1], x=emb['d2n5'][1][:,0], y=emb['d2n5'][1][:,1], hue = y_testt, palette=color_dict)\n",
    "#axes[0].figure(figsize = (10,10) )\n",
    "axes[1].axis('equal')\n",
    "axes[1].set_title(\"Embedding of test data\", fontsize=45)\n",
    "axes[1].tick_params(axis='both', which='major', labelsize=22)\n",
    "\n",
    "\n",
    "#hide legend in subplots\n",
    "for ax in axes:\n",
    "    ax.legend([],[], frameon=False)\n",
    "\n",
    "    \n",
    "# add legend\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper center',  ncol=8, bbox_to_anchor=(0.5, -0.01), fontsize=22)\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig('v2_output/prediction_plots/train_vs_test1', facecolor='white', bbox_inches='tight', dpi=100)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d2n5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "emb_full = umap.UMAP(n_neighbors=5, n_components=2, random_state=42653).fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('v2_output/svm_umap_yhat_0_f1.pkl', 'rb') as f:\n",
    "    svm_umap_yhat = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_umap_yhatt = le.inverse_transform(svm_umap_yhat['d2n5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subplots\n",
    "\n",
    "# create figure\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 10), sharey=False)\n",
    "\n",
    "\n",
    "# add subplots\n",
    "#plt.figure(figsize = (10,10) )\n",
    "sns.scatterplot(ax=axes[0], x=emb_full.embedding_[:,0], y=emb_full.embedding_[:,1], hue = labelss, palette=color_dict)\n",
    "axes[0].axis('equal')\n",
    "axes[0].set_title(\"True labels\", fontsize=45)\n",
    "axes[0].tick_params(axis='both', which='major', labelsize=22)\n",
    "\n",
    "sns.scatterplot(ax=axes[1], x=emb_full.embedding_[:,0], y=emb_full.embedding_[:,1], hue = svm_umap_yhatt, palette=color_dict)\n",
    "#axes[0].figure(figsize = (10,10) )\n",
    "axes[1].axis('equal')\n",
    "axes[1].set_title(\"Predicted labels\", fontsize=45)\n",
    "axes[1].tick_params(axis='both', which='major', labelsize=22)\n",
    "\n",
    "\n",
    "#hide legend in subplots\n",
    "for ax in axes:\n",
    "    ax.legend([],[], frameon=False)\n",
    "\n",
    "    \n",
    "# add legend\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper center',  ncol=8, bbox_to_anchor=(0.5, -0.01), fontsize=25)\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig('v2_output/prediction_plots/svm_umap_d2n5_opt1', facecolor='white', bbox_inches='tight', dpi=100)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d2n100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "emb_full = umap.UMAP(n_neighbors=100, n_components=2, random_state=42653).fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_umap_yhatt = le.inverse_transform(svm_umap_yhat['d2n100'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subplots\n",
    "\n",
    "# create figure\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 10), sharey=False)\n",
    "\n",
    "\n",
    "# add subplots\n",
    "#plt.figure(figsize = (10,10) )\n",
    "sns.scatterplot(ax=axes[0], x=emb_full.embedding_[:,0], y=emb_full.embedding_[:,1], hue = labelss, palette=color_dict)\n",
    "axes[0].axis('equal')\n",
    "axes[0].set_title(\"True labels\", fontsize=45)\n",
    "axes[0].tick_params(axis='both', which='major', labelsize=22)\n",
    "\n",
    "sns.scatterplot(ax=axes[1], x=emb_full.embedding_[:,0], y=emb_full.embedding_[:,1], hue = svm_umap_yhatt, palette=color_dict)\n",
    "#axes[0].figure(figsize = (10,10) )\n",
    "axes[1].axis('equal')\n",
    "axes[1].set_title(\"Predicted labels\", fontsize=45)\n",
    "axes[1].tick_params(axis='both', which='major', labelsize=22)\n",
    "\n",
    "\n",
    "\n",
    "#hide legend in subplots\n",
    "for ax in axes:\n",
    "    ax.legend([],[], frameon=False)\n",
    "\n",
    "    \n",
    "# add legend\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper center',  ncol=8, bbox_to_anchor=(0.5, -0.01), fontsize=25)\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig('v2_output/prediction_plots/svm_umap_d2n100_opt1', facecolor='white', bbox_inches='tight', dpi=100)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "319.99px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
